# backend/app/api/trends.py
import time
from datetime import datetime, timedelta
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import or_, delete # ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∏—è
from typing import List, Optional
from pydantic import BaseModel, Field

from ..core.database import get_db
from ..db.models import Trend
from ..services.collector import TikTokCollector
from ..services.scorer import TrendScorer
from ..services.ml_client import get_ml_client
from ..services.clustering import cluster_trends_by_visuals 

# –ò–ú–ü–û–†–¢ –ü–õ–ê–ù–ò–†–û–í–©–ò–ö–ê
from ..services.scheduler import scheduler, rescan_videos_task

router = APIRouter()

class SearchRequest(BaseModel):
    target: Optional[str] = None         # –û—Å–Ω–æ–≤–Ω–æ–π –≤–≤–æ–¥ (–∫–ª—é—á –∏–ª–∏ @username)
    keywords: Optional[List[str]] = []   # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    mode: str = "keywords"               # "keywords" –∏–ª–∏ "username"
    business_desc: Optional[str] = ""
    is_deep: Optional[bool] = False      # Light (False) vs Deep (True) Analyze
    user_tier: str = Field(default="free")  # free, creator, pro, agency
    time_window: Optional[str] = None
    rescan_hours: int = Field(default=24, ge=1)

def trend_to_dict(trend: Trend) -> dict:
    return {
        "id": trend.id,
        "platform_id": trend.platform_id,
        "url": trend.url,
        "cover_url": trend.cover_url,
        "description": trend.description,
        "author_username": trend.author_username,
        "stats": trend.stats,
        "initial_stats": trend.initial_stats, 
        "uts_score": trend.uts_score,
        "cluster_id": trend.cluster_id,       
        "music_id": trend.music_id,
        "music_title": trend.music_title,
        "last_scanned_at": trend.last_scanned_at
    }

# --- ‚úÖ –≠–ù–î–ü–û–ò–ù–¢ ¬´–ü–†–û–ß–ò–¢–ê–õ –ò –£–î–ê–õ–ò–õ¬ª ---
@router.get("/results")
def get_saved_results(keyword: str, mode: str = "keywords", db: Session = Depends(get_db)):
    """
    –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. 
    –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –ø—Ä–æ—à–ª–∏ —Ä–µ—Å–∫–∞–Ω (–¢–æ—á–∫–∞ –ë), –æ–Ω–∏ —É–¥–∞–ª—è—é—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –≤—ã–¥–∞—á–∏.
    """
    print(f"üìÇ DB Buffer Read: –∏—â–µ–º '{keyword}' –≤ —Ä–µ–∂–∏–º–µ '{mode}'")
    clean_nick = keyword.lower().strip().replace("@", "")
    
    if mode == "username":
        # –°–¢–†–û–ì–û: –∏—â–µ–º –≤–∏–¥–µ–æ, –≥–¥–µ —ç—Ç–æ—Ç —é–∑–µ—Ä —è–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–æ–º
        query = db.query(Trend).filter(Trend.author_username.ilike(clean_nick))
    else:
        # –û–ë–´–ß–ù–´–ô –ü–û–ò–°–ö: –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
        search_term = f"%{keyword}%"
        query = db.query(Trend).filter(
            or_(Trend.description.ilike(search_term), Trend.vertical.ilike(search_term))
        )
    
    results = query.order_by(Trend.uts_score.desc()).all()
    data_to_return = [trend_to_dict(t) for t in results]

    # ‚úÖ –°–ê–ú–û–û–ß–ò–°–¢–ö–ê: –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏, –µ—Å–ª–∏ —Å–≤–µ—Ä–∫–∞ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (–µ—Å—Ç—å –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∫–∞–Ω–∞)
    ids_to_clean = [t.id for t in results if t.last_scanned_at is not None]
    
    if ids_to_clean:
        db.execute(delete(Trend).where(Trend.id.in_(ids_to_clean)))
        db.commit()
        print(f"üßπ –ë–î –û—á–∏—â–µ–Ω–∞: –£–¥–∞–ª–µ–Ω–æ {len(ids_to_clean)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –≤—ã–¥–∞—á–∏.")

    return {"status": "ok", "items": data_to_return}

@router.post("/search")
def search_trends(req: SearchRequest, db: Session = Depends(get_db)):
    """
    Unified Search Endpoint with Light/Deep Analyze modes.
    
    Light Analyze (FREE/CREATOR): Basic metrics, fast results
    Deep Analyze (PRO/AGENCY): 6-layer UTS, clustering, velocity, saturation
    """
    try:
        search_targets = [req.target] if req.target else req.keywords
        if not search_targets or not search_targets[0]:
            return {"status": "error", "message": "No query provided"}
    except Exception as e:
        print(f"‚ùå Error parsing request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid request: {str(e)}")
    
    # ‚úÖ –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ê –ö DEEP ANALYZE
    if req.is_deep and req.user_tier not in ["pro", "agency"]:
        raise HTTPException(
            status_code=403,
            detail={
                "error": "Deep Analyze requires Pro plan",
                "upgrade_url": "/pricing",
                "current_tier": req.user_tier,
                "required_tier": "pro",
                "features": [
                    "6-Layer UTS Score breakdown",
                    "Visual clustering (AI)",
                    "Growth velocity prediction",
                    "Saturation indicator",
                    "Sound cascade analysis",
                    "Auto-rescan (24h tracking)"
                ]
            }
        )

    print(f"üîé API Search [{req.mode}]: {search_targets} (Mode: {'DEEP' if req.is_deep else 'LIGHT'}, Tier: {req.user_tier})")
    
    collector = TikTokCollector()
    raw_items = []
    clean_items = []
    
    # 1. –î–ª—è Light Analyze - –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (–±—ã—Å—Ç—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
    # –î–ª—è Deep Analyze - –í–°–ï–ì–î–ê –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫—ç—à (–ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
    if not req.is_deep and req.mode != "username":
        limit = 20
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            clean_nick = search_targets[0].lower().strip().replace("@", "")
            search_term = f"%{clean_nick}%"
            cached_results = db.query(Trend).filter(
                or_(Trend.description.ilike(search_term), Trend.vertical.ilike(search_term))
            ).order_by(Trend.uts_score.desc()).limit(limit).all()
        except Exception as e:
            print(f"‚ùå Error querying database cache: {e}")
            cached_results = []
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–≤–µ–∂–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–µ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if cached_results:
            recent_cached = [t for t in cached_results 
                           if not t.last_scanned_at or 
                           (datetime.utcnow() - t.last_scanned_at) < timedelta(hours=1)]
            if recent_cached:
                print(f"üíæ [LIGHT] –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à ({len(recent_cached)} –∑–∞–ø–∏—Å–µ–π)")
                return {"status": "ok", "mode": "light", "items": [trend_to_dict(t) for t in recent_cached]}
        
        # –ö—ç—à–∞ –Ω–µ—Ç - –∑–∞–ø—É—Å–∫–∞–µ–º Apify
        print(f"üîÑ [LIGHT] –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–ø—É—Å–∫–∞–µ–º Apify...")
        raw_items = collector.collect(search_targets, limit=limit, mode="search", is_deep=False)
        
        if not raw_items:
            return {"status": "empty", "items": []}
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for item in raw_items:
            v_count = int(item.get("views") or (item.get("stats") or {}).get("playCount") or 0)
            if v_count >= 5000: clean_items.append(item)
    
    # 2. –î–ª—è username –∏–ª–∏ deep scan - –≤—Å–µ–≥–¥–∞ –ø–∞—Ä—Å–∏–º
    elif req.mode == "username":
        limit = 20
        print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è '{search_targets[0]}'...")
        raw_items = collector.collect(search_targets, limit=limit, mode="profile", is_deep=True)
        if not raw_items:
            return {"status": "empty", "items": []}
        clean_items = raw_items  # –î–ª—è —é–∑–µ—Ä–∞ –±–µ—Ä–µ–º –≤—Å—ë –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    
    elif req.is_deep:
        limit = 50
        print(f"üî¨ [DEEP] Starting full analysis (no cache) –¥–ª—è '{search_targets[0]}'...")
        # –í–ê–ñ–ù–û: –î–ª—è Deep Analyze –í–°–ï–ì–î–ê –¥–µ–ª–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫—ç—à)
        raw_items = collector.collect(search_targets, limit=limit, mode="search", is_deep=req.is_deep)
        if not raw_items:
            return {"status": "empty", "items": []}
        # –î–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ
        for item in raw_items:
            v_count = int(item.get("views") or (item.get("stats") or {}).get("playCount") or 0)
            if v_count >= 5000: clean_items.append(item)

    # --- ‚úÖ –†–ï–ñ–ò–ú 1: LIGHT ANALYZE (–ë–´–°–¢–†–´–ô –ü–û–ò–°–ö –ë–ï–ó –ì–õ–£–ë–û–ö–û–ô –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò) ---
    if not req.is_deep:
        # Light Analyze: —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–∞–∫ —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        live_results = []
        for idx, item in enumerate(clean_items):
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ—Ç Apify
            v_meta = item.get("video") or item.get("videoMeta") or {}
            author_meta = item.get("author") or item.get("authorMeta") or item.get("channel") or {}
            
            # Debug: print first item structure
            if idx == 0:
                print(f"üîç DEBUG: First item keys: {list(item.keys())}")
                print(f"üîç DEBUG: video keys: {list(v_meta.keys()) if v_meta else 'EMPTY'}")
                print(f"üîç DEBUG: channel/author keys: {list(author_meta.keys()) if author_meta else 'EMPTY'}")
                print(f"üîç DEBUG: likes={item.get('likes')}, comments={item.get('comments')}, shares={item.get('shares')}")

            # Cover URL - TikTok scraper returns it in video.cover or video.dynamicCover
            cover_url = ""
            if v_meta:
                cover_url = v_meta.get("cover") or v_meta.get("coverUrl") or v_meta.get("dynamicCover") or ""
            if not cover_url:
                cover_url = item.get("coverUrl") or item.get("cover") or item.get("videoCover") or ""
            cover_url = cover_url.replace(".heic", ".jpeg").replace(".webp", ".jpeg") if cover_url else ""
            
            if idx == 0:
                print(f"üîç DEBUG: cover_url = {cover_url[:80] if cover_url else 'EMPTY'}...")

            # URL –≤–∏–¥–µ–æ
            video_url = (
                item.get("webVideoUrl") or
                item.get("postPage") or
                item.get("url") or
                item.get("videoUrl") or
                f"https://www.tiktok.com/@{author_meta.get('uniqueId', 'user')}/video/{item.get('id', '')}"
            )

            # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ —Ñ–∞–π–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
            play_addr = (
                v_meta.get("playAddr") or
                v_meta.get("downloadAddr") or
                item.get("videoUrl") or
                item.get("playAddr") or
                ""
            )

            # –û–ø–∏—Å–∞–Ω–∏–µ
            description = (
                item.get("text") or
                item.get("desc") or
                item.get("title") or
                item.get("description") or
                "No description"
            )

            # Username
            username = (
                author_meta.get("uniqueId") or
                author_meta.get("username") or
                item.get("authorName") or
                "unknown"
            )

            # Stats - TikTok scraper returns them directly in item (views, likes, comments, shares)
            # NOT in item.stats!
            stats = item.get("stats") or {}
            
            play_count = (
                item.get("views") or  # Direct from Apify - PRIORITY
                stats.get("playCount") or
                stats.get("views") or
                item.get("playCount") or
                0
            )

            # Likes/comments/shares are DIRECTLY in item for Apify TikTok scraper
            digg_count = item.get("likes") or stats.get("diggCount") or stats.get("likes") or 0
            comment_count = item.get("comments") or stats.get("commentCount") or stats.get("comments") or 0
            share_count = item.get("shares") or stats.get("shareCount") or stats.get("shares") or 0

            # Hashtags
            hashtags = item.get("hashtags") or item.get("challenges") or []
            hashtags_list = []
            if isinstance(hashtags, list):
                for tag in hashtags[:5]:  # Limit to 5 hashtags
                    if isinstance(tag, dict):
                        hashtags_list.append({
                            "id": tag.get("id") or tag.get("name", ""),
                            "name": tag.get("title") or tag.get("name", ""),
                            "title": tag.get("title") or tag.get("name", ""),
                            "desc": tag.get("desc", ""),
                            "stats": {"videoCount": 0, "viewCount": 0}
                        })

            # Music info
            music_meta = item.get("music") or item.get("musicMeta") or {}
            music_info = None
            if music_meta:
                music_info = {
                    "id": str(music_meta.get("id", "")),
                    "title": music_meta.get("title") or music_meta.get("name", "Original Sound"),
                    "authorName": music_meta.get("authorName") or music_meta.get("author", username),
                    "original": music_meta.get("original", False),
                    "playUrl": music_meta.get("playUrl", "")
                }

            # Video duration
            duration = v_meta.get("duration") or item.get("duration") or 15000  # default 15 seconds

            # Author info
            author_info = {
                "id": str(author_meta.get("id", "")),
                "uniqueId": username,
                "nickname": author_meta.get("nickname") or author_meta.get("name") or username,
                "avatar": author_meta.get("avatarThumb") or author_meta.get("avatar", ""),
                "followerCount": author_meta.get("fans") or author_meta.get("followers", 0),
                "followingCount": author_meta.get("following", 0),
                "heartCount": author_meta.get("heart", 0),
                "videoCount": author_meta.get("video") or author_meta.get("videos", 0),
                "verified": author_meta.get("verified", False)
            }

            # –ü—Ä–æ—Å—Ç–æ–π Viral Score –¥–ª—è Light —Ä–µ–∂–∏–º–∞ (–±–µ–∑ UTS –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏)
            engagement_rate = round((int(digg_count) + int(comment_count) + int(share_count)) / max(int(play_count), 1) * 100, 2) if play_count > 0 else 0
            simple_viral_score = min(engagement_rate * 10, 100)  # 0-100 scale
            
            live_results.append({
                "id": str(item.get("id", "")),
                "title": description,
                "description": description,
                "url": video_url,
                "cover_url": cover_url,
                "author_username": username,
                "play_addr": play_addr,  # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ
                "author": author_info,
                "stats": {
                    "playCount": int(play_count),
                    "diggCount": int(digg_count),
                    "commentCount": int(comment_count),
                    "shareCount": int(share_count)
                },
                "video": {
                    "duration": int(duration),
                    "ratio": "9:16",
                    "cover": cover_url,
                    "playAddr": play_addr,
                    "downloadAddr": play_addr
                },
                "music": music_info,
                "hashtags": hashtags_list,
                "createdAt": item.get("createTime") or item.get("createTimeISO", ""),
                # Light Analyze: –ø—Ä–æ—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                "viralScore": round(simple_viral_score, 1),
                "engagementRate": engagement_rate
                # –ù–ï–¢: uts_score, uts_breakdown, cluster_id, saturation_score, etc.
            })

        if len(live_results) > 0:
            print(f"‚úÖ [LIGHT MODE] Parsed {len(live_results)} items. First cover_url: {live_results[0]['cover_url'][:50] if live_results[0]['cover_url'] else 'EMPTY'}")

        return {
            "status": "ok", 
            "mode": "light",
            "items": live_results
        }

    # --- ‚úÖ –†–ï–ñ–ò–ú 2: DEEP ANALYZE (–ü–û–õ–ù–ê–Ø –ú–ê–¢–ï–ú–ê–¢–ò–ö–ê + ML) ---
    scorer = TrendScorer()
    processed_trends_objects = [] 
    cascade_total = len(clean_items)
    
    # –ü–æ–¥—Å—á–µ—Ç cascade_count –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–≤—É–∫–∞
    music_cascade_map = {}
    for item in clean_items:
        music_id = (item.get("music") or {}).get("id") or (item.get("musicMeta") or {}).get("id")
        if music_id:
            music_cascade_map[str(music_id)] = music_cascade_map.get(str(music_id), 0) + 1

    for item in clean_items:
        p_id = str(item.get("id"))
        video_url = item.get("postPage") or item.get("url") or item.get("webVideoUrl")
        views_now = int(item.get("views") or (item.get("stats") or {}).get("playCount") or 0)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è UTS —Ä–∞—Å—á–µ—Ç–∞ - Apify returns stats directly in item!
        author_meta = item.get("author") or item.get("authorMeta") or item.get("channel") or {}
        stats = item.get("stats") or {}
        
        # Stats from item directly (Apify format) or from stats object
        likes = item.get("likes") or stats.get("diggCount") or stats.get("likes") or 0
        comments = item.get("comments") or stats.get("commentCount") or stats.get("comments") or 0
        shares = item.get("shares") or stats.get("shareCount") or stats.get("shares") or 0
        bookmarks = item.get("bookmarks") or stats.get("collectCount") or stats.get("saveCount") or 0
        
        current_stats = {
            "playCount": views_now,
            "diggCount": int(likes),
            "commentCount": int(comments),
            "shareCount": int(shares)
        }
        
        followers = author_meta.get("fans") or author_meta.get("followers") or 1
        music_id = (item.get("music") or item.get("song") or {}).get("id") or (item.get("musicMeta") or {}).get("id")
        cascade_count = music_cascade_map.get(str(music_id), 1) if music_id else 1

        existing_video = db.query(Trend).filter(or_(Trend.platform_id == p_id, Trend.url == video_url)).first()

        try:
            # –†–∞—Å—á–µ—Ç UTS breakdown –¥–ª—è Deep Analyze
            # Ensure all values are integers, not None
            uts_data = {
                'views': int(views_now or 0),
                'author_followers': int(followers or 1),
                'collect_count': int(bookmarks or 0),
                'share_count': int(shares or 0),
                'likes': int(likes or 0),
                'comments': int(comments or 0)
            }
            history_data = None
            if existing_video and existing_video.initial_stats:
                history_data = {
                    'play_count': existing_video.initial_stats.get('playCount', views_now)
                }
            
            uts_breakdown = scorer.calculate_uts_breakdown(uts_data, history_data, cascade_count)
            
            if existing_video:
                # ‚úÖ –°–±—Ä–æ—Å –¢–æ—á–∫–∏ –ê –ø—Ä–∏ –Ω–æ–≤–æ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ (—Ö—Ä–∞–Ω–∏–º –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è —Å–≤–µ—Ä–∫–∏)
                existing_video.initial_stats = current_stats 
                existing_video.stats = current_stats
                existing_video.uts_score = uts_breakdown['final_score']
                existing_video.last_scanned_at = None # –û–±–Ω—É–ª—è–µ–º, —á—Ç–æ–±—ã —Ä–µ—Å–∫–∞–Ω –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–æ–≤—É—é –º–µ—Ç–∫—É
                db.add(existing_video)
                processed_trends_objects.append(existing_video)
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å ¬´–±—É—Ñ–µ—Ä–∞¬ª
                # Cover URL - check multiple locations
                v_meta = item.get("video") or item.get("videoMeta") or {}
                cover_url = (
                    v_meta.get("cover") or 
                    v_meta.get("coverUrl") or 
                    v_meta.get("dynamicCover") or 
                    item.get("coverUrl") or 
                    item.get("cover") or 
                    ""
                ).replace(".heic", ".jpeg").replace(".webp", ".jpeg")
                
                new_trend = Trend(
                    platform_id=p_id, url=video_url, 
                    cover_url=cover_url,
                    description=item.get("title") or item.get("desc") or "No desc",
                    stats=current_stats, initial_stats=current_stats,
                    author_username=(item.get("channel") or {}).get("username") or author_meta.get("uniqueId") or "unknown",
                    uts_score=uts_breakdown['final_score'], 
                    vertical=search_targets[0] or "deep_scan",
                    music_id=str(music_id) if music_id else None,
                    last_scanned_at=None
                )
                db.add(new_trend)
                processed_trends_objects.append(new_trend)
            db.commit()
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing video {p_id}: {e}")
            db.rollback()

    # 3. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø (–¢–æ–ª—å–∫–æ –¥–ª—è Deep Analyze)
    if req.is_deep and processed_trends_objects:
        print(f"üß© Starting visual clustering for {len(processed_trends_objects)} videos...")
        processed_trends_objects = cluster_trends_by_visuals(processed_trends_objects)
        for t in processed_trends_objects: db.add(t)
        try: db.commit()
        except: db.rollback()

    # 4. –ü–õ–ê–ù–ò–†–û–í–ê–ù–ò–ï –°–í–ï–†–ö–ò (24 —á–∞—Å–∞ –¥–ª—è velocity tracking)
    if req.is_deep and processed_trends_objects:
        saved_urls = [t.url for t in processed_trends_objects if t.url]
        if saved_urls:
            run_date = datetime.now() + timedelta(hours=req.rescan_hours) 
            scheduler.add_job(
                rescan_videos_task, 'date', run_date=run_date, 
                args=[saved_urls, f"batch_{int(time.time())}"]
            )
            print(f"‚è±Ô∏è AUTO-RESCAN SCHEDULED: Will run in {req.rescan_hours} hours for velocity tracking.")
    
    # 5. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å UTS breakdown
    deep_results = []
    for trend in processed_trends_objects:
        trend_dict = trend_to_dict(trend)
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º breakdown –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (—Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ stats)
        uts_data = {
            'views': trend.stats.get('playCount', 0),
            'author_followers': trend.author_followers or 1,
            'collect_count': trend.stats.get('collectCount', 0) or trend.stats.get('saveCount', 0),
            'share_count': trend.stats.get('shareCount', 0),
            'likes': trend.stats.get('diggCount', 0),
            'comments': trend.stats.get('commentCount', 0)
        }
        history_data = None
        if trend.initial_stats:
            history_data = {'play_count': trend.initial_stats.get('playCount', 0)}
        
        music_id_str = str(trend.music_id) if trend.music_id else None
        cascade_count = music_cascade_map.get(music_id_str, 1) if music_id_str else 1
        
        uts_breakdown = scorer.calculate_uts_breakdown(uts_data, history_data, cascade_count)
        
        # –î–æ–±–∞–≤–ª—è–µ–º Deep Analyze –ø–æ–ª—è
        trend_dict['uts_breakdown'] = {
            'l1_viral_lift': uts_breakdown['l1_viral_lift'],
            'l2_velocity': uts_breakdown['l2_velocity'],
            'l3_retention': uts_breakdown['l3_retention'],
            'l4_cascade': uts_breakdown['l4_cascade'],
            'l5_saturation': uts_breakdown['l5_saturation'],
            'l7_stability': uts_breakdown['l7_stability'],
            'final_score': uts_breakdown['final_score']
        }
        trend_dict['saturation_score'] = uts_breakdown['l5_saturation']
        trend_dict['cascade_count'] = cascade_count
        trend_dict['cascade_score'] = uts_breakdown['l4_cascade']
        trend_dict['velocity_score'] = uts_breakdown['l2_velocity']
        
        deep_results.append(trend_dict)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    clusters_info = {}
    for trend in processed_trends_objects:
        if trend.cluster_id is not None and trend.cluster_id >= 0:
            if trend.cluster_id not in clusters_info:
                clusters_info[trend.cluster_id] = {
                    'cluster_id': trend.cluster_id,
                    'video_count': 0,
                    'total_uts': 0,
                    'videos': []
                }
            clusters_info[trend.cluster_id]['video_count'] += 1
            clusters_info[trend.cluster_id]['total_uts'] += trend.uts_score
            clusters_info[trend.cluster_id]['videos'].append(trend.platform_id)
    
    clusters_list = []
    for cluster_id, info in clusters_info.items():
        clusters_list.append({
            'cluster_id': cluster_id,
            'video_count': info['video_count'],
            'avg_uts': round(info['total_uts'] / info['video_count'], 2) if info['video_count'] > 0 else 0
        })
    
    print(f"‚úÖ [DEEP MODE] Processed {len(deep_results)} items with full UTS breakdown. Clusters: {len(clusters_list)}")

    return {
        "status": "ok",
        "mode": "deep",
        "items": deep_results,
        "clusters": clusters_list
    }